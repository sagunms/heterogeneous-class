MP4: Reduction
==============

Objective
---------

Implement a kernel the performs reduction of a 1D list. The reduction should
give the sum of the list. You should implement the improved kernel in Lecture
5.7. Your kernel should be able to handle input lists of arbitrary length.
However, for simplicity, you can assume that the input list will be at most
2048 * 65,535 elements so that it can be handled by only one kernel launch. The
boundary condition can be handeled by filling "identity value (0 for sum)""
into the shared memory of the last block when the length is not a multiple of
the thread block size. Further assume that the reduction sums of each section
generated by individual blocks will be summed up by the CPU.

Prerequisites
-------------

Before starting this lab, make sure that:

1. You have completed MP3
2. You can have completed lecture 5.5 through 5.7

Instructions
------------

Edit the code in the code tab to perform the following:

* allocate device memory
* copy host memory to device
* initialize thread block and kernel grid dimensions
* invoke CUDA kernel
* copy results from device to host
* deallocate device memory
* implement the improved reduction routine
* use shared memory to reduce the number of global accesses, handle the
boundary conditions in when loading input list elements into the shared memory
* implement a CPU loop to perform final reduction based on the sums of sections
generated by the thread blocks

Instructions about where to place each part of the code is demarcated by the
```//@@``` comment lines.

Grading
-------

You will be grading based on the following rubric:

* Compilation (no warning): 15%
* Run time (with respect to other students): 10%
* Correctness (generates correct sum and deals with boundary conditions
correctly): 75%

If we cannot compile your program, then you will get 0 points. Note that we
only grade the last program submitted and do not accept programs beyond the
deadline.

For the run time grade, we will take the average time of all the final
submissions. If your time is no more than 10% above the average time, then you
will get full marks. Otherwise, you will lose points.

Note that the datasets that we test against are not the same as the ones
provided, so make sure to code the algorithm for correctness on general
datasets not just the ones provided.

Suggestions
-----------

* Develop your application incrementally
* Check for CUDA errors, here is some example ```wbCheck``` that you can use
  (included in the template code). using this in your code would look like
  ```wbCheck(cudaMalloc(...))```

        #define wbCheck(stmt)                                       \
            do {                                                    \
                cudaError_t err = stmt;                             \
                if (err != cudaSuccess) {                           \
                    wbLog(ERROR, "Failed to run stmt ", #stmt);     \
                    return -1;                                      \
                }                                                   \
            } while(0)

* Make sure that your algorithm handles boundary conditions where the size of
  the matrix may not be a multiple of the block/tile size
* Make sure that your algorithm handles rectangular matrices, not just square
  matrices as shown in the slides.
* Do not modify the template code written -- only insert code where the
  ```//@@``` demarcation is placed
* Do not wait until the last minute to attempt the lab
* Make sure that you test your program using all the datasets provided (the
  datasets can be selected using the dropdown next to the submission button)
* Even though you can submit multiple times, only your last submission is
graded


